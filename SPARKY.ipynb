{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SPARKY.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRtRBz1v_ZhF"
      },
      "source": [
        "**SPARKY【►_◄】V1**  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE7gD7aFN1Zn"
      },
      "source": [
        "IMPORTAMOS LIBRERIAS NECESARIAS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTGrLSZ9eVsr"
      },
      "source": [
        "import nltk  #Herramientas de lenguaje natural\n",
        "import random #Libreria para generar numeros pseudo aleatorios\n",
        "import numpy as np #librería especializada en el cálculo numérico y el análisis de datos\n",
        "import string #libreria para trabajar con cadenas"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVUWCWXC_YRa"
      },
      "source": [
        "LEYENDO DATOS "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJZQ1mtHN0nu"
      },
      "source": [
        "archivo = open('SPARKY.txt','r',errors = 'ignore')#nos permite incluir datos externos al proyecto tambien datos locales\n",
        "texto = archivo.read()#Leemos los datos importados\n",
        "texto = texto.lower()#Comvertimos a minusculas"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-OtnkiTPQ2E",
        "outputId": "9d429ceb-f947-45ed-a667-2fec93b5f01c"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "sent_tokens = nltk.sent_tokenize(texto)#Covertimos las listas en oraciones\n",
        "word_tokens = nltk.word_tokenize(texto)#Convierte una lista en palabras"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LhTarYGPQzR"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bVrqaONPQw5"
      },
      "source": [
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
        "GREETING_INPUTS = (\"HOLA\", \"hola\",\"HI\",\"HI\")\n",
        "GREETING_RESPONSES = [\"BIENBENIDO SOY SPARKY UN ASISTENTE QUE TE AYUDARA A TENER UNA VIDA MAS FELI :)   DIGAME EN QUE LE PUEDO AYUDAR\"]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sv2jVOrvPQuD"
      },
      "source": [
        "def greeting(sentence):\n",
        "    for word in sentence.split():\n",
        "        if word.lower() in GREETING_INPUTS:\n",
        "            return random.choice(GREETING_RESPONSES)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgZJXyGJPQrZ",
        "outputId": "ae8a6d0e-ea53-4b9b-fef0-ca9331944ee3"
      },
      "source": [
        "def response(user_response):\n",
        "    robo_response=''\n",
        "    sent_tokens.append(user_response)\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "    idx=vals.argsort()[0][-2]\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2]\n",
        "    if(req_tfidf==0):\n",
        "        robo_response=robo_response+\"¡Lo siento! No te entiendo\"\n",
        "        return robo_response\n",
        "    else:\n",
        "        robo_response = robo_response+sent_tokens[idx]\n",
        "        return robo_response\n",
        "flag=True\n",
        "print(\"SPARKY: Mi nombre es SPARKY. Responderé a tus consultas. Si desea salir, escriba ¡SALIR!\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SPARKY: Mi nombre es SPARKY. Responderé a tus consultas. Si desea salir, escriba ¡SALIR!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B9dcOtCPQk1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ea301c9-9dbc-404e-cbfe-4b8c25796a25"
      },
      "source": [
        "while(flag==True):\n",
        "    user_response = input()\n",
        "    user_response=user_response.lower()\n",
        "    if(user_response!='salir'):\n",
        "        if(user_response=='GRACIAS' or user_response=='GRACIAS' ):\n",
        "            flag=False\n",
        "            print(\"SPARKY: DE NADA :) ...\")\n",
        "        else:\n",
        "            if(greeting(user_response)!=None):\n",
        "                print(\"SPARKY: \"+greeting(user_response))\n",
        "            else:\n",
        "                print(\"SPARKY: \",end=\"\")\n",
        "                print(response(user_response))\n",
        "                sent_tokens.remove(user_response)\n",
        "    else:\n",
        "        flag=False\n",
        "        print(\"SPARKY: ¡ADIOS! CUIDATE :) ...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hola\n",
            "SPARKY: BIENBENIDO SOY SPARKY UN ASISTENTE QUE TE AYUDARA A TENER UNA VIDA MAS FELI :)   DIGAME EN QUE LE PUEDO AYUDAR\n",
            "tengo un problema\n",
            "SPARKY: "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tengo un prblema\n",
            "estamos para escucharlo puede comnetarnos como se siente.\n",
            "me siento mal\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SPARKY: mal\n",
            "nadie te puede hacer sentir inferior sin tu consentimiento.\n",
            "estoy en un estado critico\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SPARKY: critico\n",
            "por tu bien te reomendamos visitar un especiaista.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}